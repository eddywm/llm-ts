# ğŸ§  LLM-TS - 100k params AI model

> ğŸš€ **Large Language Model Transformer implemented in TypeScript**

An experimental transformer architecture built entirely in TypeScript for educational purposes and research exploration.

## âœ¨ Features

- ğŸ”¤ **Token Embeddings** - Convert text to numerical representations
- ğŸ“ **Positional Encoding** - Add position information to token embeddings
- ğŸ¯ **Multi-Head Self-Attention** - Core attention mechanism with parallel heads
- ğŸ”„ **Feed-Forward Networks** - GELU activation with layer transformations
- ğŸ”§ **Layer Normalization** - Stabilize training with normalization layers
- ğŸ”— **Residual Connections** - Skip connections for better gradient flow
- ğŸ­ **Causal Masking** - Enable autoregressive text generation
- âš¡ **Optimized Matrix Operations** - Custom operations for transformer architecture

## âœ¨ Screenshots

<img width="676" height="783" alt="Screenshot 2025-08-25 at 7 12 04 PM" src="https://github.com/user-attachments/assets/d06001aa-76e8-4f48-bab2-17ceda5c2603" />


## ğŸ› ï¸ Technology Stack

- **Language**: TypeScript
- **Architecture**: Transformer
- **Paradigm**: Experimental/Educational

## ğŸ¯ Purpose

This project serves as a learning resource and experimental playground for understanding transformer architectures at a fundamental level. Built from scratch in TypeScript to provide clear, readable implementations of key transformer components.

## ğŸš€ Getting Started

```bash
# Clone the repository
git clone https://github.com/eddywm/llm-ts.git

# Navigate to project directory
cd llm-ts

# Install dependencies
npm install

# Run the project
npm start
```

## ğŸ—ï¸ Architecture Components

```
ğŸ›ï¸ Transformer Model
â”œâ”€â”€ ğŸ“ Token Embeddings
â”œâ”€â”€ ğŸ“ Positional Encoding
â”œâ”€â”€ ğŸ¯ Multi-Head Attention
â”œâ”€â”€ ğŸ”„ Feed-Forward Network
â”œâ”€â”€ ğŸ”§ Layer Normalization
â””â”€â”€ ğŸ­ Causal Masking
```

## ğŸ’¡ Use Cases

- ğŸ“ Educational exploration of transformer internals
- ğŸ”¬ Research and experimentation
- ğŸ“š Understanding LLM fundamentals
- ğŸ› ï¸ Custom model development

## ğŸ¤ Contributing

Contributions are welcome! This is an experimental project aimed at learning and exploration.

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“„ License

This project is available under the MIT License.

## ğŸ™‹â€â™‚ï¸ Author

**eddywm** - [GitHub Profile](https://github.com/eddywm)

---

â­ **Star this repo if you find it helpful for learning about transformers!** â­
