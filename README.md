# 🧠 LLM-TS - 100k params AI model

> 🚀 **Large Language Model Transformer implemented in TypeScript**

An experimental transformer architecture built entirely in TypeScript for educational purposes and research exploration.

## ✨ Features

- 🔤 **Token Embeddings** - Convert text to numerical representations
- 📍 **Positional Encoding** - Add position information to token embeddings
- 🎯 **Multi-Head Self-Attention** - Core attention mechanism with parallel heads
- 🔄 **Feed-Forward Networks** - GELU activation with layer transformations
- 🔧 **Layer Normalization** - Stabilize training with normalization layers
- 🔗 **Residual Connections** - Skip connections for better gradient flow
- 🎭 **Causal Masking** - Enable autoregressive text generation
- ⚡ **Optimized Matrix Operations** - Custom operations for transformer architecture

## ✨ Screenshots

<img width="676" height="783" alt="Screenshot 2025-08-25 at 7 12 04 PM" src="https://github.com/user-attachments/assets/d06001aa-76e8-4f48-bab2-17ceda5c2603" />


## 🛠️ Technology Stack

- **Language**: TypeScript
- **Architecture**: Transformer
- **Paradigm**: Experimental/Educational

## 🎯 Purpose

This project serves as a learning resource and experimental playground for understanding transformer architectures at a fundamental level. Built from scratch in TypeScript to provide clear, readable implementations of key transformer components.

## 🚀 Getting Started

```bash
# Clone the repository
git clone https://github.com/eddywm/llm-ts.git

# Navigate to project directory
cd llm-ts

# Install dependencies
npm install

# Run the project
npm start
```

## 🏗️ Architecture Components

```
🏛️ Transformer Model
├── 📝 Token Embeddings
├── 📍 Positional Encoding
├── 🎯 Multi-Head Attention
├── 🔄 Feed-Forward Network
├── 🔧 Layer Normalization
└── 🎭 Causal Masking
```

## 💡 Use Cases

- 🎓 Educational exploration of transformer internals
- 🔬 Research and experimentation
- 📚 Understanding LLM fundamentals
- 🛠️ Custom model development

## 🤝 Contributing

Contributions are welcome! This is an experimental project aimed at learning and exploration.

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## 📄 License

This project is available under the MIT License.

## 🙋‍♂️ Author

**eddywm** - [GitHub Profile](https://github.com/eddywm)

---

⭐ **Star this repo if you find it helpful for learning about transformers!** ⭐
